{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, zero_one_loss\n",
    "\n",
    "%config InlineBackend.figure_format='svg'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "covertype_path = './covtype/covtype.data'\n",
    "\n",
    "names = [\n",
    "    'Elevation',\n",
    "    'Aspect',\n",
    "    'Slope',\n",
    "    'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology',\n",
    "    'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am',\n",
    "    'Hillshade_Noon',\n",
    "    'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points',\n",
    "]\n",
    "\n",
    "Wilderness_Area = []\n",
    "for i in range(4):\n",
    "    Wilderness_Area.append('Wilderness_Area_{}'.format(i+1))\n",
    "\n",
    "Soil_Type = []\n",
    "for i in range(40):\n",
    "    Soil_Type.append('Soil_Type_{}'.format(i+1))\n",
    "\n",
    "names.extend(Wilderness_Area + Soil_Type + ['Cover_Type'])\n",
    "covertype_df = pd.read_csv(covertype_path, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <th>Soil_Type_40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       2596      51      3                               258   \n",
       "1       2590      56      2                               212   \n",
       "2       2804     139      9                               268   \n",
       "3       2785     155     18                               242   \n",
       "4       2595      45      2                               153   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               0                              510   \n",
       "1                              -6                              390   \n",
       "2                              65                             3180   \n",
       "3                             118                             3090   \n",
       "4                              -1                              391   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            221             232            148   \n",
       "1            220             235            151   \n",
       "2            234             238            135   \n",
       "3            238             238            122   \n",
       "4            220             234            150   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points     ...      Soil_Type_32  Soil_Type_33  \\\n",
       "0                                6279     ...                 0             0   \n",
       "1                                6225     ...                 0             0   \n",
       "2                                6121     ...                 0             0   \n",
       "3                                6211     ...                 0             0   \n",
       "4                                6172     ...                 0             0   \n",
       "\n",
       "   Soil_Type_34  Soil_Type_35  Soil_Type_36  Soil_Type_37  Soil_Type_38  \\\n",
       "0             0             0             0             0             0   \n",
       "1             0             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   Soil_Type_39  Soil_Type_40  Cover_Type  \n",
       "0             0             0           5  \n",
       "1             0             0           5  \n",
       "2             0             0           2  \n",
       "3             0             0           2  \n",
       "4             0             0           5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covertype_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = covertype_df.drop('Cover_Type',axis=1)\n",
    "y = covertype_df['Cover_Type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(covertype_df.as_matrix(), test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58101, 55)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from math import log\n",
    "import random\n",
    "\n",
    "def two_split(index, value, dataset):\n",
    "    left = []\n",
    "    right = []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def entropy(dataset, classes):\n",
    "    size = len(dataset)\n",
    "    if size == 0:\n",
    "        return 0\n",
    "    ent = 0.0\n",
    "    count = [0.0] * len(classes)\n",
    "    for data in dataset:\n",
    "        count[classes.index(data[-1])] += 1\n",
    "    for c in count:\n",
    "        proportion = c / size\n",
    "        if proportion != 0:\n",
    "            ent += -proportion * log(proportion, 2)\n",
    "    return ent\n",
    "        \n",
    "    \n",
    "    '''    \n",
    "    for cls in classes:\n",
    "        count = 0.0\n",
    "        for data in dataset:\n",
    "            if data[-1] == cls:\n",
    "                count += 1;\n",
    "        proportion = count / size\n",
    "        if proportion != 0:\n",
    "            ent += -proportion * log(proportion, 2)\n",
    "    return ent'''\n",
    "\n",
    "\n",
    "def entropy_compute(groups, classes):\n",
    "    dataset = []\n",
    "    for group in groups:\n",
    "        dataset += group\n",
    "    # information for this node\n",
    "    ent_I = entropy(dataset, classes)\n",
    "    # information required for subtrees / groups\n",
    "    ent_remain = 0.0\n",
    "    for group in groups:\n",
    "        ent_temp = entropy(group, classes)\n",
    "        ent_remain += (len(group)*1.0 / len(dataset)) * ent_temp\n",
    "    return (ent_I - ent_remain)\n",
    "\n",
    "\n",
    "def get_split(dataset, n_features):\n",
    "    #X data, y label, n_feartures number of features you want\n",
    "    features = [];\n",
    "    classes = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 0, 0, None\n",
    "    while b_groups is None:\n",
    "        while len(features) < n_features:\n",
    "            index = randrange(len(dataset[0]) - 1)\n",
    "            if index not in features:\n",
    "                features.append(index)\n",
    "        for index in features:\n",
    "            # get value range\n",
    "            vals = set([row[index] for row in dataset])\n",
    "            vals = list(vals)\n",
    "            # if less than 10 classes, just use it\n",
    "            # else compute 8 cuts\n",
    "            if len(vals) > 10:\n",
    "                low, high = min(vals), max(vals)\n",
    "                diff = (high - low) / 21\n",
    "                vals = [x * diff + low for x in range(0, 21)]\n",
    "            for value in vals:\n",
    "                groups = two_split(index, value, dataset)\n",
    "                ent = entropy_compute(groups, classes)\n",
    "                if ent > b_score:\n",
    "                    b_index, b_value, b_score, b_groups = \\\n",
    "                        index, value, ent, groups\n",
    "        '''        \n",
    "        for row in dataset:\n",
    "            # for quicker choosing\n",
    "            if random.random() < 0.99:\n",
    "                continue\n",
    "            groups = two_split(index, row[index], dataset)\n",
    "            ent = entropy_compute(groups, classes)\n",
    "            if ent > b_score:\n",
    "                b_index, b_value, b_score, b_groups = \\\n",
    "                    index, row[index], ent, groups\n",
    "        '''\n",
    "        if b_groups == None:\n",
    "            for index in features:\n",
    "                # get value range\n",
    "                vals = set([row[index] for row in dataset])\n",
    "                vals = list(vals)\n",
    "                # if less than 10 classes, just use it\n",
    "                # else compute 8 cuts\n",
    "                if len(vals) > 10:\n",
    "                    low, high = min(vals), max(vals)\n",
    "                    diff = (high - low) / 11\n",
    "                    vals = [(x * diff + low) for x in range(0, 11)]\n",
    "                print \"here the values\", vals\n",
    "                for value in vals:\n",
    "                    groups = two_split(index, value, dataset)\n",
    "                    ent = entropy_compute(groups, classes)\n",
    "                    if ent > b_score:\n",
    "                        b_index, b_value, b_score, b_groups = \\\n",
    "                            index, value, ent, groups\n",
    "                print \"the entropy is\", ent\n",
    "    return {'index':b_index, 'value':b_value, 'groups': b_groups}\n",
    "\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key = outcomes.count)\n",
    "\n",
    "# creat child split\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    #print \"the depth is\", depth\n",
    "    # check terminate\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left\n",
    "    classes = list(set(row[-1] for row in (left + right)))\n",
    "    if len(left) <= min_size or entropy(left, classes) < 0.01:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    # process right,\n",
    "    if len(right) <= min_size or entropy(right, classes) < 0.01:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "# build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "# prediction\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "        \n",
    "# random sample -- bootstrap\n",
    "def resample(dataset, ratio):\n",
    "    sample = list()\n",
    "    num = round(len(dataset) * ratio)\n",
    "    while len(sample) < num:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "# predict\n",
    "def bagging_predict(trees, row):\n",
    "    decisions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(decisions), key = decisions.count)\n",
    "\n",
    "# Random Forest\n",
    "def random_forest(train, test, max_depth, min_size\\\n",
    "                 , sample_ratio, n_trees, n_features):\n",
    "    trees = []\n",
    "    for i in range(n_trees):\n",
    "        samples = resample(train, sample_ratio)\n",
    "        tree = build_tree(samples, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree number 1\n"
     ]
    }
   ],
   "source": [
    "# build tree with max_depth 5\n",
    "trees = []\n",
    "n_trees = 2\n",
    "sample_ratio = 0.2\n",
    "max_depth = 5\n",
    "min_size = 10\n",
    "n_features = 0.4 * len(train[0])\n",
    "\n",
    "for i in range(n_trees):\n",
    "    print(\"tree number \" + str(i + 1))\n",
    "    samples = resample(train, sample_ratio)\n",
    "    \n",
    "    tree = build_tree(samples, max_depth, min_size, n_features)\n",
    "    trees.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees_full = []\n",
    "n_trees = 100\n",
    "sample_ratio = 0.2\n",
    "max_depth = 100\n",
    "min_size = 25\n",
    "n_features = 0.4 * len(train[0])\n",
    "\n",
    "for i in range(n_trees):\n",
    "    print(\"tree number \" + str(i + 1))\n",
    "    samples = resample(train, sample_ratio)\n",
    "    tree = build_tree(samples, max_depth, min_size, n_features)\n",
    "    trees_full.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38037 14637    15     0     1     0   316]\n",
      " [16286 53455  1016     0    19     0     0]\n",
      " [    0  1263  7646     0     0     0     0]\n",
      " [    0     1   650     0     0     0     0]\n",
      " [    0  2209    70     0   194     0     0]\n",
      " [    0  1222  3151     0     0     0     0]\n",
      " [ 3713    24     0     0     0     0  1328]]\n"
     ]
    }
   ],
   "source": [
    "# test predictions for trees depth 5\n",
    "predictions = [bagging_predict(trees, row) for row in X_test];\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pylab as pl\n",
    "accuracy = confusion_matrix(y_test,predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions for trees full\n",
    "predictions2 = [bagging_predict(trees_full, row) for row in X_test];\n",
    "\n",
    "accuracy = confusion_matrix(y_test,predictions2)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot error rate\n",
    "# error rate method\n",
    "def get_error_rate(pred, Y):\n",
    "    return sum(pred != Y) / float(len(Y))\n",
    "# plot the error rate\n",
    "def plot_error_rate2(er_train, er_test):\n",
    "    df_error = pd.DataFrame([er_train, er_test]).T\n",
    "    df_error.columns = ['Training', 'Test']\n",
    "    plot1 = df_error.plot(linewidth = 3, figsize = (8,6),\n",
    "            color = ['lightblue', 'darkblue'], grid = True)\n",
    "    plot1.set_xlabel('Number of iterations', fontsize = 12)\n",
    "    plot1.set_xticklabels(range(0,100,20))\n",
    "    plot1.set_ylabel('Error rate', fontsize = 12)\n",
    "    plot1.set_title('Error rate vs number of iterations', fontsize = 16)\n",
    "    plt.axhline(y=er_test[0], linewidth=1, color = 'red', ls = 'dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# investigate the number of trees\n",
    "nums = [1, 20, 40, 60, 80, 100]\n",
    "err_train = []\n",
    "err_test = []\n",
    "for n in nums:\n",
    "    # compute number of errors in each number of trees\n",
    "    pred_train = [bagging_predict(trees[0:n], row) for row in train]\n",
    "    pred_test = [bagging_predict(trees[0:n], row) for row in test]\n",
    "    err_train.append(get_error_rate(pred_train, train[:, -1]))\n",
    "    err_test.append(get_error_rate(pred_test, test[:, -1]))\n",
    "plot_error_rate2(err_train, err_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# investigate the number of trees\n",
    "nums = [1, 20, 40, 60, 80, 100]\n",
    "err_train = []\n",
    "err_test = []\n",
    "for n in nums:\n",
    "    # compute number of errors in each number of trees\n",
    "    pred_train = [bagging_predict(trees_full[0:n], row) for row in train]\n",
    "    pred_test = [bagging_predict(trees_full[0:n], row) for row in test]\n",
    "    err_train.append(get_error_rate(pred_train, train[:, -1]))\n",
    "    err_test.append(get_error_rate(pred_test, test[:, -1]))\n",
    "plot_error_rate2(err_train, err_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error_rate(pred, Y):\n",
    "    return sum(pred != Y) / float(len(Y))\n",
    "\n",
    "#print the error rate\n",
    "def print_error_rate(err):\n",
    "    print 'Error rate: Training: %.4f - Test: %.4f' % err\n",
    "\n",
    "#get the error rate for train and test\n",
    "def generic_clf(X_train, y_train, X_test, y_test, clf):\n",
    "    clf=clf.fit(X_train,y_train.reshape(-1,1))\n",
    "    pred_train = clf.predict(X_train)\n",
    "    pred_test = clf.predict(X_test)\n",
    "    return get_error_rate(pred_train, y_train), \\\n",
    "           get_error_rate(pred_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_features(dataset):\n",
    "    info_gain = [];\n",
    "    classes = list(set(row[-1] for row in dataset))\n",
    "    features = range(len(dataset[0]))\n",
    "    for index in features:            \n",
    "        # get value range\n",
    "        print \"feature: \", index\n",
    "        vals = set([row[index] for row in dataset])\n",
    "        vals = list(vals)\n",
    "        if len(vals) > 25:\n",
    "            lo = min(vals)\n",
    "            hi = max(vals)\n",
    "            diff = (hi - lo) / 50.0\n",
    "            vals = [x*diff + lo for x in range(1,50)]\n",
    "        max_ent = 0;\n",
    "        for row in dataset:\n",
    "            for value in vals:\n",
    "                groups = two_split(index, value, dataset)\n",
    "                ent = entropy_compute(groups, classes)\n",
    "                if ent > max_ent:\n",
    "                    max_ent = ent\n",
    "        info_gain.append(max_ent)\n",
    "    feat_gain = [(i, info_gain[i]) for i in range(len(features))]\n",
    "    feat_gain_sorted = sorted(feat_gain, key= lambda tup: tup[1])\n",
    "    print \"feature - information gain\"\n",
    "    for row in feat_gain_sorted:\n",
    "        print row\n",
    "    return feat_gain_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_features(test[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_gain_sorted=rank_features1(train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_gain_sorted=feat_gain_sorted[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb=[]\n",
    "cc=[]\n",
    "for i in range(0,53):\n",
    "    bb.append(temp[i][0])\n",
    "    cc.append(temp[i][1])\n",
    "print bb,cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "\n",
    "y_pos = np.arange(len(bb))    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(y_pos, cc, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, bb)\n",
    "plt.ylabel('Feature importance')\n",
    "plt.title('Feature importance')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_gain_sorted=feat_gain_sorted[1:-1]\n",
    "temp = sorted(feat_gain_sorted, key= lambda tup: tup[1],reverse=True)\n",
    "bb=[]\n",
    "cc=[]\n",
    "for i in range(0,20):\n",
    "    bb.append(temp[i][0])\n",
    "    cc.append(temp[i][1])\n",
    "print bb,cc\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "\n",
    "y_pos = np.arange(len(bb))    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(y_pos, cc, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, bb)\n",
    "plt.ylabel('Feature importance')\n",
    "plt.title('Feature importance')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def adaboost_clf2(y_train, X_train, y_test, X_test, M, Max_depth, Min_split):\n",
    "    n_train, n_test = len(X_train), len(X_test)\n",
    "    err_test, err_train = [], []\n",
    "    classes = list(set(y_train))\n",
    "    n_class = len(classes)\n",
    "    # initialize weights\n",
    "    w = np.ones(n_train) / n_train\n",
    "    pred_train, pred_test = np.zeros((n_train, n_class)), np.zeros((n_test, n_class))\n",
    "    clf = DecisionTreeClassifier(max_depth=Max_depth, min_samples_split=Min_split, random_state=42)\n",
    "    for i in range(M):\n",
    "        \n",
    "        clf.fit(X_train, y_train, sample_weight = w)\n",
    "        pred_train_i = clf.predict(X_train)\n",
    "        pred_test_i = clf.predict(X_test)\n",
    "        \n",
    "        miss1 = [int(x) for x in (pred_train_i != y_train)]\n",
    "        err_m = np.dot(w,miss1) / sum(w)\n",
    "        #print err_m\n",
    "        alpha_m = np.log((1-err_m) / float(err_m)) + math.log(n_class - 1)\n",
    "        #print alpha_m\n",
    "        \n",
    "        miss2 = [x if x == 1 else -1 for x in miss1]\n",
    "        w = np.multiply(w, np.exp([float(x) * alpha_m for x in miss2]))\n",
    "        \n",
    "        #print miss1\n",
    "        #print miss2\n",
    "        #print w\n",
    "        # update\n",
    "        for i in range(n_train):\n",
    "            cls = pred_train_i[i]\n",
    "            idx = classes.index(cls)\n",
    "            pred_train[i, idx] += alpha_m\n",
    "        for i in range(n_test):\n",
    "            cls = pred_test_i[i]\n",
    "            idx = classes.index(cls)\n",
    "            pred_test[i, idx] += alpha_m\n",
    "                                     \n",
    "        prediction_train = [classes[np.argmax(row)] for row in pred_train]\n",
    "        #print 'the prediction', prediction_train\n",
    "        #print 'the y train', y_train\n",
    "        prediction_test = [classes[np.argmax(row)] for row in pred_test]\n",
    "        err_train.append(get_error_rate(prediction_train, y_train))\n",
    "        err_test.append(get_error_rate(prediction_test, y_test))\n",
    "    return err_train, err_test,prediction_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_train, err_test,prediction_test=adaboost_clf2(y_train, X_train, y_test, X_test, 100, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_error_rate(er_train, er_test):\n",
    "    df_error = pd.DataFrame([er_train, er_test]).T\n",
    "    df_error.columns = ['Training', 'Test']\n",
    "    plot1 = df_error.plot(linewidth = 3, figsize = (8,6),\n",
    "            color = ['lightblue', 'darkblue'], grid = True)\n",
    "    plot1.set_xlabel('Number of iterations', fontsize = 12)\n",
    "    plot1.set_xticklabels(range(0,500,20))\n",
    "    plot1.set_ylabel('Error rate', fontsize = 12)\n",
    "    plot1.set_title('Error rate vs number of iterations', fontsize = 16)\n",
    "    plt.axhline(y=er_test[0], linewidth=1, color = 'red', ls = 'dashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy1 = confusion_matrix(y_test,prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['1', '2','3','4','5','6','7']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(accuracy1, classes=labels,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(accuracy1, classes=labels,normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=mnist_train_df.ix[1:785,1:].T.values\n",
    "y_train=mnist_train_df.ix[0,1:].values\n",
    "X_test=mnist_train_df.ix[1:785,1:].T.values\n",
    "y_test=mnist_train_df.ix[0,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_train, err_test,prediction_test=adaboost_clf2(y_train, X_train, y_test, X_test, 100, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_error_rate(err_train, err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
